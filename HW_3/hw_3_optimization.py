# -*- coding: utf-8 -*-
"""HW_3_Optimization.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BB8qw8IkitXjATt1gXP22oIjT1Ou21XG

## ДЗ№3: "Методы оптимизации"

ФИО: **Тренёв Иван Сергеевич**
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np

import torch
from tqdm.auto import tqdm
from torch import nn
import torch.nn.functional as F
from torchvision import datasets, transforms
from torch.optim import SGD as torchSGD
from torch.optim import Adagrad as torchAdagrad
from torch.optim import RMSprop as torchRMSprop
from torch.optim import Adadelta as torchAdadelta
from torch.optim import Adam as torchAdam

import matplotlib.pyplot as plt
from matplotlib.pyplot import cm
from matplotlib.colors import LogNorm 
# %matplotlib inline

"""### Часть 1: Реализация методов

Полезная функция: plt.contour
Для всех экспериментов подберите параметры так, чтобы метод сошелся к ближайшему локальному минимуму. Все методы следует запускать из одной и той же точки.

<i> 1.1 Реализуйте методы GD, Momentum, NAG, Adagrad, Adadelta, Adam.</i>
"""

class Optimizer:
    def __init__(self, params):
        self.params = list(params)

    def zero_grad(self):
        for param in self.params:
            if param.grad is not None:
                param.grad.detach_()
                param.grad.zero_()
    
    def pre_step(self):
        pass
    
    def step(self):
        pass

def optimize_function(fn, optim, optim_args, start_point, num_iter = 50):
    weigths = nn.Parameter(torch.FloatTensor(start_point), requires_grad=True)

    optim = optim(params=[weigths], **optim_args)
    points = []
    losses = []
    for i in range(num_iter):
        if hasattr(optim, 'pre_step'):
            optim.pre_step()
        loss = fn(weigths[0], weigths[1])
        points.append(weigths.data.detach().clone())
        losses.append(loss.item())
        loss.backward()
        optim.step()
        optim.zero_grad()
    points = torch.stack(points, axis=0)
    losses = torch.FloatTensor(losses)
    return points, losses

def compare_optimizers(
    fn,
    optim_list,
    start_point,
    x_range=(-5, 5),
    y_range=(-5, 5),
    xstep=0.2,
    ystep=0.2,
    minima=None,
    num_iter = 50,
):
    xmin, xmax = x_range
    ymin, ymax = y_range
    x, y = np.meshgrid(np.arange(xmin, xmax + xstep, xstep), np.arange(ymin, ymax + ystep, ystep))
    z = fn(torch.from_numpy(x), torch.from_numpy(y))
    z = z.detach().numpy()
    fig, ax = plt.subplots(figsize=(10, 6))

    ax.contour(x, y, z, levels=np.logspace(0, 5, 35), norm=LogNorm(), cmap=plt.cm.jet)
    if minima:
        ax.plot(*minima, 'r*', markersize=18)

    ax.set_xlabel('$x$')
    ax.set_ylabel('$y$')

    ax.set_xlim((xmin, xmax))
    ax.set_ylim((ymin, ymax))
    
    colors = cm.rainbow(np.linspace(0, 1, len(optim_list)))
    
    iter = 0
    for c, (name, optim, args) in zip(colors, optim_list):
        points, losses = optimize_function(fn, optim, args, start_point, num_iter)
        ax.quiver(
            points[:-1, 0], points[:-1, 1],  
            points[1:, 0] - points[:-1, 0], points[1:, 1] - points[:-1, 1], 
            scale_units='xy', angles='xy', scale=1, color=c,
            label=name#, width=0.008
        )
        iter += 1
        if (iter % 2 == 0):
          print(f"{name}-loss: {losses.numpy()[-1]}", end='\n\n')
        else:
          print(f"{name}-loss: {losses.numpy()[-1]}", end='\n')

    
    ax.legend()
    plt.show()

class SGD(Optimizer):
    def __init__(self, params, lr=1e-2):
        super().__init__(params)
        self.lr = lr
        
    def step(self):
        with torch.no_grad():
            for p in self.params:
                if p.grad is None:
                    continue
                p -= self.lr * p.grad

class Momentum(Optimizer):
    def __init__(self, params, lr=1e-2, gamma=0.9):
        super().__init__(params)
        self.lr = lr
        self.gamma = gamma
        self.v = [torch.zeros_like(p) for p in self.params]
    def step(self):
        with torch.no_grad():
            for v, p in zip(self.v, self.params):
                if p.grad is None:
                    continue
                v.copy_(self.gamma * v + self.lr * p.grad)
                p -= v

class NAG(Optimizer):
    def __init__(self, params, lr=1e-2, gamma=0.9):
        super().__init__(params)
        self.lr = lr
        self.gamma = gamma
        self.v = [torch.zeros_like(p) for p in self.params]
    
    def pre_step(self):
        with torch.no_grad():
            for v, p in zip(self.v, self.params):
                p -= self.gamma * v
    def step(self):
        with torch.no_grad():
            for v, p in zip(self.v, self.params):
                if p.grad is None:
                    continue
                p += self.gamma * v
                v.copy_(self.gamma * v + self.lr * p.grad)
                p -= v

class Adagrad(Optimizer):
    def __init__(self, params, lr=1e-2, eps=1e-10):
        super().__init__(params)
        self.lr = lr
        self.eps = eps
        self.G = [torch.zeros_like(p) for p in self.params]
    
    def step(self):
        with torch.no_grad():
            for G, p in zip(self.G, self.params):
                if p.grad is None:
                    continue
                G.copy_(G + p.grad ** 2)
                p -= self.lr * p.grad / (torch.sqrt(G) + self.eps)

class RMSProp(Optimizer):
    def __init__(self, params, lr=1e-2, gamma=0.99, eps=1e-8):
        super().__init__(params)
        self.lr = lr
        self.eps = eps
        self.gamma = gamma
        self.g = [torch.zeros_like(p) for p in self.params]
    
    def step(self):
        with torch.no_grad():
            for g, p in zip(self.g, self.params):
                if p.grad is None:
                    continue
                g.copy_(self.gamma * g + (1 - self.gamma) * p.grad ** 2)
                p -= self.lr * p.grad / torch.sqrt(g + self.eps)

class Adadelta(Optimizer):
    def __init__(self, params, gamma=0.9, eps=1e-6):
        super().__init__(params)
        self.eps = eps
        self.gamma = gamma
        self.g = [torch.zeros_like(p) for p in self.params]
        self.dp = [torch.zeros_like(p) for p in self.params]
    
    def step(self):
        with torch.no_grad():
            for g, dp, p in zip(self.g, self.dp, self.params):
                if p.grad is None:
                    continue
                g.copy_(self.gamma * g + (1 - self.gamma) * p.grad ** 2)
                delta_p = p.grad * torch.sqrt(dp + self.eps) / torch.sqrt(g + self.eps)
                dp.copy_(self.gamma * dp + (1 - self.gamma) * delta_p ** 2)
                p -= delta_p

class Adam(Optimizer):
    def __init__(self, params, lr=1e-3, beta_1=0.9, beta_2=0.999, eps=1e-8):
        super().__init__(params)
        self.lr = lr
        self.beta_1 = beta_1
        self.beta_2 = beta_2
        self.eps = eps
        self.m = [torch.zeros_like(p) for p in self.params]
        self.v = [torch.zeros_like(p) for p in self.params]
    
    def step(self):
        with torch.no_grad():
            iter = 0
            for v, m, p in zip(self.v, self.m, self.params):
                iter += 1
                if p.grad is None:
                    continue
                m.copy_(self.beta_1 * m + (1 - self.beta_1) * p.grad)
                v.copy_(self.beta_2 * v + (1 - self.beta_2) * (p.grad ** 2))

                m_h = m / (1 - self.beta_1 ** iter)
                v_h = v / (1 - self.beta_2 ** iter)

                p -= self.lr * m_h / (torch.sqrt(v_h) + self.eps)

"""<i> 1.2 Сравните эти методы на функции $J(x, y) = x^2+y^2$</i>"""

def F1(x, y):
    return x**2 + y**2

compare_optimizers(
    F1,
    [
        ('SGD', SGD, dict(lr=1e-2)),
        ('torch_SGD', torchSGD, dict(lr=1e-2)),
     
        ('Momentum', Momentum, dict(lr=1e-3)),
        ('torch_Momentum', torchSGD, dict(lr=1e-3, momentum=0.9)),
     
        ('nag', NAG, dict(lr=1e-3)),
        ('torch_nag', torchSGD, dict(lr=1e-3, momentum=0.9, nesterov=True)),
     
        ('RMSProp', RMSProp, dict(lr=1e-1)),
        ('torch_RMSProp', torchRMSprop, dict(lr=1e-1)), 
     
        ('Adagrad', Adagrad, dict(lr=0.5)),
        ('torch_Adagrad', torchAdagrad, dict(lr=0.5)),
     
        ('Adam', Adam, dict(lr=1e-1, beta_1=0.9, beta_2=0.999)),
        ('torch_adam', torchAdam, {'lr': 1e-1, 'betas': (0.9, 0.999)}),
     
        ('Adadelta', Adadelta, dict(gamma=0.1, eps=1e-6)),
        ('torch_Adadelta', torchAdadelta, dict(rho=0.1, eps=1e-6))
    ],
    start_point=[3, 3],
    minima=(0, 0),
    num_iter=300
)

"""<i>1.3 Сравните эти методы на функции $J(x, y) = x^2sin(x)+y^2sin(y)$</i>"""

def F2(x, y):
    return torch.sin(x)*x**2 + torch.sin(y)*y**2

compare_optimizers(
    F2,
    [     
        ('SGD', SGD, dict(lr=1.21e-4)),
        ('torch_SGD', torchSGD, dict(lr=1.21e-4)),
     
        ('Momentum', Momentum, dict(lr=1.33e-6, gamma=0.99)),
        ('torch_Momentum', torchSGD, dict(lr=1.33e-6, momentum=0.99)),
     
        ('nag', NAG, dict(lr=1.33e-6, gamma=0.99)),
        ('torch_nag', torchSGD, dict(lr=1.33e-6, momentum=0.99, nesterov=True)),
     
        ('RMSProp', RMSProp, dict(lr=3.1e-4)),
        ('torch_RMSProp', torchRMSprop, dict(lr=3.1e-4)), 
     
        ('Adagrad', Adagrad, dict(lr=6.16e-3)),
        ('torch_Adagrad', torchAdagrad, dict(lr=6.16e-3)),
     
        ('Adam', Adam, dict(lr=3.058e-4, beta_1=0.9, beta_2=0.999)),
        ('torch_adam', torchAdam, {'lr': 3.058e-4, 'betas': (0.9, 0.999)}),
     
        ('Adadelta', Adadelta, dict(gamma=0.895, eps=1e-9)),
        ('torch_Adadelta', torchAdadelta, dict(rho=0.895, eps=1e-9))
    ],
    start_point=[2, -1],
    minima=(2.2, 2.3),
    num_iter=1500
)

"""<i>1.3 Сравните эти методы на функции $J(x,y)=x^2sin(x^2)+y^2sin(y^2)$</i>"""

def F3(x, y):
    return torch.sin(x**2)*x**2 + torch.sin(y**2)*y**2

compare_optimizers(
    F3,
    [
        ('SGD', SGD, dict(lr=3e-3)),
        ('torch_SGD', torchSGD, dict(lr=3e-3)),
     
        ('Momentum', Momentum, dict(lr=1e-3)),
        ('torch_Momentum', torchSGD, dict(lr=1e-3, momentum=0.9)),
     
        ('nag', NAG, dict(lr=1e-3)),
        ('torch_nag', torchSGD, dict(lr=1e-3, momentum=0.9, nesterov=True)),
     
        ('RMSProp', RMSProp, dict(lr=1e-1)),
        ('torch_RMSProp', torchRMSprop, dict(lr=1e-1)), 
     
        ('Adagrad', Adagrad, dict(lr=5e-2)),
        ('torch_Adagrad', torchAdagrad, dict(lr=5e-2)),
     
        ('Adam', Adam, dict(lr=1e-1, beta_1=0.9, beta_2=0.999)),
        ('torch_adam', torchAdam, {'lr': 1e-1, 'betas': (0.9, 0.999)}),
     
        ('Adadelta', Adadelta, dict(gamma=0.8, eps=1e-6)),
        ('torch_Adadelta', torchAdadelta, dict(rho=0.8, eps=1e-6))
    ],
    start_point=[1, 1],
    minima=(0, 0),
    num_iter=1300
)

"""###  Часть 2: Обучение нейронной сети"""

!wget www.di.ens.fr/~lelarge/MNIST.tar.gz
!tar -zxvf MNIST.tar.gz

"""Формируем обучающий и тестовый датасеты"""

transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.1307,), (0.3081,))
])

transform_aug = transforms.Compose([
    transforms.RandomRotation(degrees = 15), # вращения
    transforms.RandomAffine(degrees = 0, translate = (0.1, 0.1)), # сдвиги
    transforms.ToTensor(),
    transforms.Normalize((0.1307,), (0.3081,))
])

train_dataset = datasets.MNIST('.', train=True, download=True, transform=transform_aug)
test_dataset = datasets.MNIST('.', train=False, transform=transform)

train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=True)

"""<i> 2.2 Сравните графики обучения для сверточной нейросети на методах Adam, Adagrad, AdaDelta и SGD. Для обучения используйте оптимизаторы из первой части, а не из pytorch. </i>

Architecture
"""

class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        # Convolutional Layers
        self.conv1 = nn.Conv2d(1, 10, kernel_size=3)
        self.conv2 = nn.Conv2d(10, 20, kernel_size=3)
        self.conv3 = nn.Conv2d(20, 30, kernel_size=5)
        # Zero random channels
        self.conv3_drop = nn.Dropout2d(0.25)
        # Line Layers
        self.fc1 = nn.Linear(480, 50)
        self.fc2 = nn.Linear(50, 10)

    def forward(self, x):
        # Pooling with relu
        x = F.relu(F.max_pool2d(self.conv2(self.conv1(x)), kernel_size=2))
        x = F.relu(F.max_pool2d(self.conv3_drop(self.conv3(x)), kernel_size=2))
        # To vec
        x = x.view(-1, 480)
        x = F.relu(self.fc1(x))
        # Zero random elements
        x = F.dropout(x)
        x = self.fc2(x)
        return F.log_softmax(x, -1)

def train(train_losses):
  # train mode
  network.train()
  train_loss = []
  correct = 0
  for batch_idx, (data, target) in enumerate(train_loader):
    optimizer.zero_grad()

    output = network(data)
    pred = output.data.max(1, keepdim=True)[1]
    correct += pred.eq(target.data.view_as(pred)).sum()

    loss = F.nll_loss(output, target)
    loss.backward()

    optimizer.step()    
    if batch_idx % log_interval == 0:
      train_loss.append(loss.item())

  train_losses.append(np.mean(train_loss))
  print(f"Train: Avg. loss: {np.mean(train_loss):.4f}, Accuracy: {correct / len(train_loader.dataset)}.")

def test(test_losses):
  # test mode
  network.eval()
  test_loss = []
  correct = 0
  with torch.no_grad():
    for data, target in test_loader:
      output = network(data)
      pred = output.data.max(1, keepdim=True)[1]
      correct += pred.eq(target.data.view_as(pred)).sum()

      test_loss.append(F.nll_loss(output, target).item())

  test_losses.append(np.mean(test_loss))
  print(f"Test: Avg. loss: {np.mean(test_loss):.4f}, Accuracy: {correct / len(test_loader.dataset)}\n\n")

def realization(optimizer, n_epochs=10, log_interval=10, train_losses=[], test_losses=[]):

  for epoch in range(1, n_epochs + 1):
    print(f"Epoch: {epoch}")
    train(train_losses)
    test(test_losses)

  plt.figure(figsize=(10, 8))
  plt.plot(train_losses, label='Train')
  plt.plot(test_losses, label='Test')
  plt.xlabel('Epochs', fontsize=16)
  plt.ylabel('Loss', fontsize=16)
  plt.legend(loc=0, fontsize=16)
  plt.grid('on')
  plt.show()

n_epochs = 30
log_interval = 10

train_losses_SGD = []
test_losses_SGD = []
network = Net()
optimizer = SGD(network.parameters(), lr=4e-3)

realization(optimizer, n_epochs, log_interval, train_losses_SGD, test_losses_SGD)

n_epochs = 30
log_interval = 10

test_losses_Adagrad = []
train_losses_Adagrad = []
network = Net()
optimizer = Adagrad(network.parameters(), lr=2e-3)

realization(optimizer, n_epochs, log_interval, train_losses_Adagrad, test_losses_Adagrad)

n_epochs = 9
log_interval = 15

test_losses_Adadelta = []
train_losses_Adadelta = []
network = Net()
optimizer = Adadelta(network.parameters(), eps=1e-8)

realization(optimizer, n_epochs, log_interval, train_losses_Adadelta, test_losses_Adadelta)

n_epochs = 30
log_interval = 10

test_losses_Adam = []
train_losses_Adam = []
network = Net()
optimizer = Adam(network.parameters(), lr=1e-3)

realization(optimizer, n_epochs, log_interval, train_losses_Adam, test_losses_Adam)