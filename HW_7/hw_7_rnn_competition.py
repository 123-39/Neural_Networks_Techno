# -*- coding: utf-8 -*-
"""HW_7_RNN_Competition.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oraTVwGUd1c8IvZUBXP0hCswo6pFlQ8d
"""

!pip install pmdarima

# Commented out IPython magic to ensure Python compatibility.
import random
import numpy as np
import pandas as pd
import os

from tqdm.notebook import tqdm
import matplotlib.pyplot as plt
# %matplotlib inline

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
from torch.utils.tensorboard import SummaryWriter

from sklearn.metrics import mean_squared_error

import pmdarima as pm

from google.colab import drive
drive.mount('/content/drive')

"""# Ноутбук для ДЗ №4 обработка временных последовательностей RNN
В данном дз вам будут данных времменая последовательно, которая описывает распределение хитов по времени за несколько лет. Вам нужно будет обучить модель RNN на исторических данных и потом сделать предсказание для "будущего" года, в текущей задаче это 2019 год

## Готовим данные
Три файла с данными
1. исторические данные - train
2. Тестовые данные текущего момента - derived
3. Пример сабмита резкльтатов конкурса на кагле
"""

def read_set(file):
    data = pd.read_csv(file)
    data['date'] = pd.to_datetime(data['date'])
    data = data.rename(columns = {'DATE':'date'})
    data = data.rename(columns = {'hits':'value'})
    data = data.set_index('date')
    return data

data_train = read_set('/content/drive/MyDrive/Techno_NN_HW7/train.csv')
data_test =  read_set('/content/drive/MyDrive/Techno_NN_HW7/derived.csv')
data_sample = read_set('/content/drive/MyDrive/Techno_NN_HW7/sample_submission.csv')

data_train.head()

"""### Проверим наши данные, что мы загрузили"""

print(data_train.info())
print("="*53)
print(data_test.info())

data_train.describe()

data_test.describe()

"""## Графики наших временных последовательностей"""

plt.figure(figsize=(12,8))
data_train['value'].plot(kind = 'line')
data_test['value'].plot(kind = 'line')
plt.show()

"""## Статистическая модель [ARIMA](https://ru.wikipedia.org/wiki/ARIMA)

"""

# обучаем модель
arima_model=model = pm.auto_arima(data_train, seasonal = True, m = 4, test='adf',
                                  error_action='ignore', suppress_warnings=True,
                                  stepwise=True, trace=True)

prediction = pd.DataFrame(model.predict(n_periods = int(data_test.size)), data_test.index)
prediction = prediction.rename(columns = {0:'value'})

"""смотрим, что она нам предсказала"""

plt.figure(figsize=(12,8))
data_train['value'].plot(kind = 'line')
data_test['value'].plot(kind = 'line')
#plt.plot(data_forecaste, label = "Prediction")
prediction['value'].plot(kind = 'line')
plt.show()

"""### Функция подсчета метрик для конкурса"""

def MAPE(y_true, y_pred):
    mape = np.abs(y_pred - y_true) / np.maximum(np.abs(y_true), 1e-6)
    mape  = np.average(mape) * 100
    return mape

"""### MAPE для ARIMA и тестового сабмишена

## Из пандас строим датасет
"""

def MAPE(y_true, y_pred):
    mape = np.abs(y_pred - y_true) / np.maximum(np.abs(y_true), 1e-6)
    mape  = np.average(mape) * 100
    return mape

class TSDataset(Dataset):
    
    def __init__(self, data, seq_len):
        super().__init__()
        self._len = len(data) - seq_len + 1 
        self.mean = np.mean(data)
        self.std = np.std(data)
        self.data = (data - self.mean) / self.std 
        self.seq_len = seq_len
        
    def __len__(self):
        return self._len
    
    def __getitem__(self, idx):
        d = self.data[idx:idx + self.seq_len]
        targets = []
        days   = []
        months = []
        year = []
        for row in d.iterrows():
            targets += [row[1]['value'] ]
            days += [ row[0].day ]
            months += [row[0].month]
            year += [row[0].year]
            
        return torch.LongTensor(days), \
               torch.LongTensor(months), \
               torch.LongTensor(year), \
               torch.FloatTensor(targets)

"""## Теперь нужно определить нашу модель """

def seed_everything(seed: int):

    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = True
    
seed_everything(777)

class TimeSeriesModel(nn.Module):
        def __init__(self, hidden_size: int, input_sizes: tuple,
                     num_layers: int, dropout: float):            
            super().__init__()
            self.mon_emb = nn.Embedding(12 + 1, input_sizes[0])
            self.day_emb = nn.Embedding(31 + 1, input_sizes[1])
            
            self._rnn = nn.GRU(input_size=(input_sizes[0] + input_sizes[1] + 1),  
                               hidden_size=hidden_size,
                               num_layers=num_layers,
                               dropout=dropout,
                               batch_first=True)

            self._output = nn.Linear(hidden_size, 1)
            
        def forward(self, batch, ctx = None):
            days, mons, _, targets = batch
            mon_tensor = self.mon_emb(mons)
            day_tensor = self.day_emb(days)
            rnn_input  = torch.cat([mon_tensor, day_tensor], dim=-1)
            targets = targets.unsqueeze(-1)
            rnn_input = torch.cat([rnn_input, targets], dim=-1)
            rnn_input = rnn_input[:, :-1, :] if ctx is None else rnn_input
            output, ctx = self._rnn(rnn_input, ctx)
            output = self._output(output).squeeze()
            return output, ctx

# модель обучаем в режиме teacher forcing, т.е. на вход подаем сразу всю последовательность,
# на выходе таргет должен быть смещен на один временной шаг, чтобы правильно считался лосс
def train(global_epoch):
    for epoch in range(0, global_epoch):
        epoch_iter = tqdm(dl_train)
        series_model.train()
        for batch in epoch_iter:
            optimizer.zero_grad()
            # Чтобы сохранялась временная зависимость
            # для предсказания таргет должен быть смешен на один временной шаг
            # относительно входа модели
            target = batch[-1][:,1:]
            result, _ = series_model(batch)    
            batch_loss = loss(result, target)
            batch_loss.backward()
            epoch_iter.set_description(f"Epoch: {epoch},\t Iter Loss: {batch_loss:.3f}")
            optimizer.step()
        with torch.no_grad():
            series_model.eval()
            res = test_model(epoch)
        global_epoch += 1
    return res

def test_model(epoch):
    test_iter = tqdm(dl_test)
    sum_loss = 0
    ans = []
    for idx, batch in enumerate(test_iter):
        # Чтобы сохранялась временная зависимость
        # для предсказания таргет должен быть смешен на один временной шаг
        # относительно входа модели
        target = batch[-1][:, 1:]
        result, _ = series_model(batch)
        batch_loss = loss(result, target)
        sum_loss += batch_loss
        if idx == 0:
          ans = result[:, 0].detach().tolist()
        elif idx != int(len(ds_test) / BATCH_SIZE):
          ans.extend(result[:, 0].detach().tolist())
        else:
          ans.extend(torch.cat([result[:, 0], result[-1, 1:]]).detach().tolist())
    sum_loss /= len(dl_test)
    print(f"Epoch: {epoch}, Val Loss: {sum_loss:.3f}")
    # scheduler.step(sum_loss)
    scheduler.step()
    return ans

"""### Определяем даталоадеры для теста и трейна"""

SEQ_LEN = 30
BATCH_SIZE = 32
HIDDEN_SIZE = 216
EMB_SIZE = (3, 3)
NUM_LAYERS = 2
DROPOUT = 0.2

lr = 1e-3

ds_train = TSDataset(data_train, SEQ_LEN)
ds_test  = TSDataset(data_test, SEQ_LEN)


dl_train = DataLoader(ds_train, batch_size=BATCH_SIZE, shuffle=True)
dl_test = DataLoader(ds_test, batch_size=BATCH_SIZE, shuffle=False)

loss = nn.L1Loss()
series_model = TimeSeriesModel(HIDDEN_SIZE, EMB_SIZE, NUM_LAYERS, DROPOUT)
optimizer = optim.Adam(series_model.parameters(), lr=lr)
scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)

"""### Обучаем модель"""

global_epoch = 10
global_iter = 0

pred_test = train(global_epoch)

# сохраняем модель
torch.save(series_model.state_dict(), '/content/drive/MyDrive/Techno_NN_HW7/series_model2.ptx')

# восстанавливаем модель
series_model = TimeSeriesModel(HIDDEN_SIZE, EMB_SIZE, NUM_LAYERS, DROPOUT)
series_model.load_state_dict(torch.load('/content/drive/MyDrive/Techno_NN_HW7/series_model2.ptx'))

"""### TODO
Теперь нам нужно для нашего тестового сета сгенерировать результат, и сделать правильный сабмишен. В отличии от режима обучения мы не должгны использовать значения таргетов в тесте, поэтому нам придется тут реализовывать инкрементальный режим генерации сети, т.е. когда на вход подаются фичи и таргет с предыдущего шага на каждоим шаге генерации.
"""

with open('/content/drive/MyDrive/Techno_NN_HW7/submission.csv', 'wt') as sf:
    print('date,hits', file=sf)
    with torch.no_grad():
        train_last_window = DataLoader(ds_train, 1 , False)
        last_window = None
        for b in train_last_window:
            last_window = b

        series_model.eval()
        ds_val = TSDataset(data_test, 1)
        output, ctx = series_model(last_window)
        d, m, year, prev = last_window[0][0,-1:], last_window[1][0,-1:], last_window[2][0,-1:],  output[-1:]
        for i in range(len(ds_val)):
            output, ctx = series_model((d.unsqueeze(0), m.unsqueeze(0), 0, prev.unsqueeze(0)), ctx)
            d, m, year, prev = ds_val[i][0], ds_val[i][1], ds_val[i][2], torch.FloatTensor([output])
            print('%d-%02d-%02d,%d' %(year,m,d, float(prev)*ds_train.std + ds_train.mean), file=sf)
    sf.close()

data_nn_submission = read_set('/content/drive/MyDrive/Techno_NN_HW7/submission.csv')
MAPE(data_test, data_nn_submission)

plt.figure(figsize=(12,8))
data_train['value'].plot(kind = 'line')
data_test['value'].plot(kind = 'line')
data_nn_submission['value'].plot(kind = 'line')
plt.show()