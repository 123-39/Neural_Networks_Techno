# -*- coding: utf-8 -*-
"""HW_8_AdvancedNLP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cYgihdpZZhK_eXPbw4Ktm02B4u7YXJhi

## ДЗ№8: "Современные модели для NLP"

ФИО: **Тренёв Иван Сергеевич**

### На семинаре мы разберем [код трансфомера на pytorch](https://nlp.seas.harvard.edu/2018/04/03/attention.html)

###  ДЗ [3 балла]

Обратите внимание, что в этой работе вам потребуется скачать модель весом ~150MB, также ее вычисление занимает определенное время, так что рекомендуется считать эту задачу на [google colab](https://colab.research.google.com/).
"""

import numpy as np
import torch
!pip install --upgrade transformers
!pip install sentencepiece
from transformers import *

MODEL = (MobileBertForMaskedLM, MobileBertTokenizer, 'google/mobilebert-uncased')

model_class, tokenizer_class, pretrained_weights = MODEL
# Load pretrained model/tokenizer
tokenizer = tokenizer_class.from_pretrained(pretrained_weights)
model = model_class.from_pretrained(pretrained_weights)

input_ids = tokenizer.encode("Here is some text to encode", add_special_tokens=True)  # Add special tokens takes care of adding [CLS], [SEP], <s>... tokens in the right way for each model.
print(input_ids)

tokenizer.decode(input_ids)

input_ids[8] = tokenizer.mask_token_id
tokenizer.decode(input_ids)

input_batch = torch.tensor(input_ids).unsqueeze(0) # batch_size 1
with torch.no_grad():
    res = model(input_batch)[0]

prob = torch.nn.functional.softmax(res, dim=-1)
new_ids = prob.max(-1)[1]

new_ids

tokenizer.decode(new_ids.numpy()[0, :].tolist())

GPT_TEXTS = [
    "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.",
    "A train carriage containing controlled nuclear materials was stolen in Cincinnati today. Its whereabouts are unknown."
    ]

"""Ваша задача - сгенерировать продолжение текстов, на которых демонстрировалась работа GPT-2 с помощью загруженной модели (DistillBERT). Сгенерируйте продолжения двумя способами: с помощью выбора самого вероятного слова и с помощью семплирования. Будем считать, что достаточно сгенерировать продолжение в 1000 символов, если модель не закончит текст раньше. Также можно попробовать сравнить эту генерацию с какой-нибудь легковесной gpt, например, "sshleifer/tiny-gpt2".

BERT не предназначен для генерации текста, но есть возможность предсказывать замаскированное \[MASK\] слово. Попробуем составить неполное предложение и добавить фальшивую маску в конец (что по своей сути является предсказанием следующего слова).

### BERT
"""

tokenizer = DistilBertTokenizer.from_pretrained("bert-base-uncased")
model = TFDistilBertForMaskedLM.from_pretrained("bert-base-uncased")

def bert_generation(text):
    all_text = [text]
    len_text = len(text)
    sentence = ('{} [MASK]'.format(text))
    while len_text <= 1000:
        
        indices = tokenizer.encode(sentence, add_special_tokens=False, return_tensors='tf')
        prediction = model(indices)
        masked_indices = np.where(indices==103)[1]

        output = np.argmax(np.asarray(prediction[0][0])[masked_indices,:] ,axis=1)
        new_word = tokenizer.decode(output)
        # check PAD token
        if new_word == tokenizer.decode(0):
            break
        all_text.append(' ' + new_word)
        new_text = ''.join(all_text)
        len_text = len(new_text)
        sentence = ('{} [MASK]'.format(new_text))

    return ''.join(all_text)

bert_generation(GPT_TEXTS[0])

bert_generation(GPT_TEXTS[1])

"""### GPT2"""

tokenizer = GPT2Tokenizer.from_pretrained("sshleifer/tiny-gpt2")
model = GPT2LMHeadModel.from_pretrained("sshleifer/tiny-gpt2", pad_token_id=tokenizer.eos_token_id)

def gpt_generation(user_text, size=1000):
    # Tokenizer user_text
    tokens = tokenizer.encode(user_text, return_tensors="pt")

    # Generate
    # max_length = context_length + output_length
    tokens = model.generate(tokens, max_length=size+tokens.shape[1], do_sample=True, top_k=50)
    tokens = tokens[0].tolist()
    return tokenizer.decode(tokens, skip_special_tokens=True)

generate(GPT_TEXTS[0])

gpt_generation(GPT_TEXTS[1])

"""#### Feedback (опционально)

Здесь вы можете оставить список опечаток из лекции или семинара:

Здесь вы можете оставить комментарии по лекции или семинару:
"""