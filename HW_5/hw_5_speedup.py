# -*- coding: utf-8 -*-
"""HW_5_Speedup.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IqOzntstPS-5OG2fBd4sYHhDQshmZFBu

## ДЗ №5: "Улучшение сходимости нейросетей"

ФИО: Тренёв Иван Сергеевич
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import torch
from torch import nn
import torch.nn.functional as F
from torch.autograd import Variable
from torch.utils.data import DataLoader
from torchvision.datasets import MNIST
from torchvision import transforms
# %matplotlib inline

from IPython.display import clear_output

def _epoch(network, loss, loader,
           backward=True,
           optimizer=None,
           device='cpu',
           ravel_init=False):
    losses = []
    accuracies = []
    for X, y in loader:
        X = X.to(device)
        y = y.to(device)
        if ravel_init:
            X = X.view(X.size(0), -1)
        network.zero_grad()
        prediction = network(X)
        loss_batch = loss(prediction, y)
        losses.append(loss_batch.cpu().item())
        if backward:
            loss_batch.backward()
            optimizer.step()
        prediction = prediction.max(1)[1]
        accuracies.append((prediction==y).cpu().float().numpy().mean())
    return losses, accuracies


def train(network, train_loader, test_loader,
          epochs, learning_rate, ravel_init=False,
          device='cpu', tolerate_keyboard_interrupt=True):
    loss = nn.NLLLoss()
    optimizer = torch.optim.Adam(network.parameters(), lr=learning_rate)
    train_loss_epochs = []
    test_loss_epochs = []
    train_accuracy_epochs = []
    test_accuracy_epochs = []
    network = network.to(device)
    try:
        for epoch in range(epochs):
            network.train()
            losses, accuracies = _epoch(network,
                                        loss,
                                        train_loader,
                                        True,
                                        optimizer,
                                        device,
                                        ravel_init)
            train_loss_epochs.append(np.mean(losses))
            train_accuracy_epochs.append(np.mean(accuracies))
            
            network.eval()
            losses, accuracies = _epoch(network,
                                        loss,
                                        test_loader,
                                        False,
                                        optimizer,
                                        device,
                                        ravel_init)

            test_loss_epochs.append(np.mean(losses))
            test_accuracy_epochs.append(np.mean(accuracies))
            clear_output(True)
            print('Epoch {0}... (Train/Test) NLL: {1:.3f}/{2:.3f}\tAccuracy: {3:.3f}/{4:.3f}'.format(
                        epoch, train_loss_epochs[-1], test_loss_epochs[-1],
                        train_accuracy_epochs[-1], test_accuracy_epochs[-1]))
            plt.figure(figsize=(12, 5))
            plt.subplot(1, 2, 1)
            plt.plot(train_loss_epochs, label='Train')
            plt.plot(test_loss_epochs, label='Test')
            plt.xlabel('Epochs', fontsize=16)
            plt.ylabel('Loss', fontsize=16)
            plt.legend(loc=0, fontsize=16)
            plt.grid()
            plt.subplot(1, 2, 2)
            plt.plot(train_accuracy_epochs, label='Train accuracy')
            plt.plot(test_accuracy_epochs, label='Test accuracy')
            plt.xlabel('Epochs', fontsize=16)
            plt.ylabel('Accuracy', fontsize=16)
            plt.legend(loc=0, fontsize=16)
            plt.grid()
            plt.show()
    except KeyboardInterrupt:
        if tolerate_keyboard_interrupt:
            pass
        else:
            raise KeyboardInterrupt
    return train_loss_epochs, \
           test_loss_epochs, \
           train_accuracy_epochs, \
           test_accuracy_epochs

"""На этом семинаре мы попробуем улучшить результаты, полученные на предыдущем занятии
Для этого нам понадобятся следующие вещи:
* Dropout
* Batch Normalization
* Инициализация весов

### Часть 1: Инициализация весов

На лекции доказывалось, что при инициализации He и Glorot дисперсия активаций градиентов в каждом слое будут примерно равны. Давайте проверим это.
"""

# Dataloader
to_numpy = lambda x: x.numpy()
transform = transforms.Compose([
                       transforms.ToTensor(),
                       transforms.Normalize((0.1307,), (0.3081,))
                    ])
train_dataset = MNIST('.', train=True, download=True, transform=transform)
test_dataset = MNIST('.', train=False, transform=transform)

train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=128, shuffle=True)

images_train, labels_train = next(iter(train_loader))

## Usage example:
for X, y in train_loader:
    X = X.view(X.size(0), -1)
    X = X.numpy() ### Converts torch.Tensor to numpy array
    y = y.numpy()
    pass

plt.figure(figsize=(6, 7))
for i in range(25):
    plt.subplot(5, 5, i+1)
    plt.imshow(X[i].reshape(28, 28), cmap=plt.cm.Greys_r)
    plt.title(y[i])
    plt.axis('off')

"""<i> 1.1 </i> Инициализируйте полносвязную сеть нормальным шумом N(0, 0.1) с архитектурой 784 -> 500 x (10 раз) -> 10. В качестве активации возьмите tanh"""

def init_layer(layer, mean=0, std=1):
    weight = layer.state_dict()['weight'].normal_(mean=mean, std=std)
    layer.state_dict()['bias'].zero_()

def init_layer_uniform_dist(layer, a):
    nn.init.uniform_(layer.state_dict()['weight'], -a, a)
    layer.state_dict()['bias'].zero_()
    
def Normal(size_input, size_output):
    return 0.1

def Xavier(size_input, size_output):
    d = 4 / 3 * np.sqrt(6) / np.sqrt(size_input + size_output)
    return d

def He(size_input, size_output):
    d = 2 / size_output
    return np.sqrt(d)


def forward_hook(self, input_, output):
    std = input_[0].std().item()
    var_activ.append(std)

def backward_hook(self, grad_input, grad_output):
    std = grad_input[0].std().item()
    var_grad.append(std)

def nn_initialize(sizes, init_func, activation):

    layers = []
    for size_input, size_output in zip(sizes[:-1], sizes[1:]):
      
      layer = nn.Linear(size_input, size_output)
      layer.register_forward_hook(forward_hook)
      layer.register_backward_hook(backward_hook)
      if init_func.__name__ == 'Xavier':
          init_layer_uniform_dist(layer, init_func(size_input, size_output))
      else:
          init_layer(layer, 0.0, init_func(size_input, size_output))

      layers.append(layer)
      layers.append(activation)

    layers.pop()

    return layers

"""<i>1.2 Пропустите батч изображений через нейронную сеть и вычислите дисперсию активаций. Затем вычислите градиент и получите дисперсию градиентов. Сравните эти значения между собой для разных слоев.</i>"""

def batch_check_grad(var_activ, var_grad):
    n_objects = 100
    X = images_train[:n_objects].view(n_objects, -1).data
    y = labels_train[:n_objects].data
    activations = network(X)
    loss_fn = torch.nn.NLLLoss()
    # optimizer = torch.optim.Adam(network.parameters(), lr=0.001) 
    loss = loss_fn(activations, y)
    loss.backward()

    plt.figure(figsize=(12, 5))
    plt.subplot(1, 2, 1)
    plt.plot(range(len(sizes) - 1), var_activ)
    plt.xlabel('Layer', fontsize=16)
    plt.ylabel('Activation variance', fontsize=16)
    plt.grid()
    plt.subplot(1, 2, 2)
    plt.plot(range(len(sizes) - 1), var_grad)
    plt.xlabel('Layer', fontsize=16)
    plt.ylabel('Gradient variance', fontsize=16)
    plt.grid()
    plt.show()

sizes = [784] + [500] * 30 + [10]
init_func = Normal
activation = nn.Tanh()
var_activ, var_grad = [], []

NN_normal_noise = nn_initialize(sizes, init_func, activation)
network = nn.Sequential(*NN_normal_noise)

batch_check_grad(var_activ, var_grad)

"""<i>1.3 Повторите эксперимент для инициализаций He и Xavier (формулы есть в лекции).</i>"""

sizes = [784] + [500] * 30 + [10]
init_func = Xavier
activation = nn.Tanh()
var_activ, var_grad = [], []

NN_normal_noise = nn_initialize(sizes, init_func, activation)
network = nn.Sequential(*NN_normal_noise)

batch_check_grad(var_activ, var_grad)

sizes = [784] + [500] * 30 + [10]
init_func = He
activation = nn.Tanh()
var_activ, var_grad = [], []

NN_normal_noise = nn_initialize(sizes, init_func, activation)
network = nn.Sequential(*NN_normal_noise)

batch_check_grad(var_activ, var_grad)

"""### Часть 2: Dropout

Другим полезным слоем является Dropout. В нем с вероятностью 1-p зануляется выход каждого нейрона. Этот слой уже реализован в pyTorch, поэтому вновь реализовывать его не интересно. Давайте реализуем DropConnect — аналог Dropout. В нем с вероятностью 1-p зануляется каждый вес слоя.

<i> 2.1 Реализуйте линейный слой с DropConnect </i>
"""

# полезная функция: .bernoulli_(p)
# не забывайте делать requires_grad=False у маски
# помните, что в вычислениях должны участвовать Variable, а не тензоры

class DropConnect(nn.Module):
    def __init__(self, input_dim, output_dim, p=0.5):
        super(DropConnect, self).__init__()
        self.linear = nn.Linear(input_dim, output_dim)
        self.p = p

    def forward(self, x):    
        mask = torch.ones_like(self.linear.weight)
        if self.training:          
            mask.bernoulli_(self.p)

        mask = mask.data
        output = F.linear(x, self.linear.weight * mask, self.linear.bias)
        return output

"""<i> 
2.2 Сравните графики обучения нейроных сетей:
1. Свертки из TestNetwork -> 128 -> 128 -> 10 с ReLU и Dropout между всеми слоями 
2. Свертки из TestNetwork -> 128 -> 128 -> 10 с ReLU DropConnect вместо всех линейных слоев
</i>
"""

class TestNetwork(nn.Module):
    def __init__(self, final_part):
        super().__init__()    
        
        channels = 1
        
        self.conv_layers = nn.Sequential(
            nn.Conv2d(channels, 2, 3, padding=1),    
            nn.MaxPool2d(2),
            nn.ReLU(),
            nn.Conv2d(2, 4, 3, padding=1),            
            nn.MaxPool2d(2),
            nn.ReLU(),
        )
        
        self.flatten = nn.Flatten()
        
        self.final_part = final_part
        
        self.log_softmax = nn.LogSoftmax(1)        
        
    def forward(self, x):
        x = self.conv_layers(x)
        x = self.flatten(x)
        x = self.final_part(x)
        return self.log_softmax(x)

sizes = [196, 128, 128, 10]
layers_dropout = []
for size_input, size_output in zip(sizes, sizes[1:]):
    layer = nn.Linear(size_input, size_output)
    layers_dropout.append(layer)
    layers_dropout.append(nn.Dropout(0.3))
    layers_dropout.append(nn.ReLU())

# delete last activation
layers_dropout.pop()
# delete last dropout
layers_dropout.pop()

network_dropout = TestNetwork(nn.Sequential(*layers_dropout))

dropout_output = train(network_dropout, train_loader, test_loader, 50, 0.001, 
                       device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'))

sizes = [196, 128, 128, 10]
layers_dropconnect = []
for size_input, size_output in zip(sizes, sizes[1:]):
    # last layer without dropconnect
    if size_output == sizes[-1]:
        layer = nn.Linear(size_input, size_output)
    else:
        layer = DropConnect(size_input, size_output, 0.8)
    layers_dropconnect.append(layer)
    layers_dropconnect.append(nn.ReLU())

layers_dropconnect.pop()

layers_dropconnect = TestNetwork(nn.Sequential(*layers_dropconnect))

dropconnect_output = train(layers_dropconnect, train_loader, test_loader, 50, 0.001, 
                           device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'))

"""В test-time стохастичность Dropout убирают и заменяют все веса на их ожидаемое значение: $\mathbb{E}w = pw + (1-p)0 = pw$.

### Часть 3: Batch Normalization

Наконец, давайте рассмотрим Batch Normalization. Этот слой вычитает среднее и делит на стандартное отклонение. Среднее и дисперсия вычисляются по батчу независимо для каждого нейрона. У этого слоя есть две важные проблемы: его нельзя использовать при обучении с размером батча 1 и он делает элементы батча зависимыми. Давайте реализуем аналог батч нормализации: <a href=https://arxiv.org/pdf/1607.06450.pdf>Layer normalization</a>. В layer normalization среднее и дисперсия вычисляются по активациям нейронов, независимо для каждого объекта.

<i> 3.1 Реализуйте Layer Normalization </i>
"""

class LayerNormalization(nn.Module):

    def __init__(self, normal_shape, gamma=True, beta=True, epsilon=1e-5):
        super(LayerNormalization, self).__init__()
        if isinstance(normal_shape, int):
            normal_shape = (normal_shape,)
        else:
            normal_shape = (normal_shape[-1],)
        self.normal_shape = torch.Size(normal_shape)
        self.epsilon = epsilon

        if gamma:
            self.gamma = nn.Parameter(torch.Tensor(*normal_shape))
        else:
            self.register_parameter('gamma', None)
            
        if beta:
            self.beta = nn.Parameter(torch.Tensor(*normal_shape))
        else:
            self.register_parameter('beta', None)

        self.reset_parameters()

    def reset_parameters(self):
        if self.gamma is not None:
            self.gamma.data.fill_(1)
        if self.beta is not None:
            self.beta.data.zero_()

    def forward(self, x):
        mean = x.mean(dim=-1, keepdim=True)
        var = x.var(dim=-1, keepdim=True)
        std = (var + self.epsilon).sqrt()
        y = (x - mean) / std
        if self.gamma is not None:
            y *= self.gamma
        if self.beta is not None:
            y += self.beta
        return y

"""<i> 
3.2 Сравните графики обучения нейроных сетей:
1. Свертки из TestNetwork -> 128 -> 128 -> 10 с ReLU и Batch normalization между всеми слоями 
2. Свертки из TestNetwork -> 128 -> 128 -> 10 с ReLU и Layer normalization между всеми слоями 
</i>
"""

sizes = [196, 128, 128, 10]
layers_layer_norm = []
for size_input, size_output in zip(sizes, sizes[1:]):
    layer = nn.Linear(size_input, size_output)
    layers_layer_norm.append(layer)
    layers_layer_norm.append(LayerNormalization(size_output))
    layers_layer_norm.append(nn.ReLU())

# delete last activation
layers_dropout.pop()
# delete last layer normalization
layers_dropout.pop()

network_dropout = TestNetwork(nn.Sequential(*layers_layer_norm))

dropout_output = train(network_dropout, train_loader, test_loader, 50, 0.001, 
                       device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'))

sizes = [196, 128, 128, 10]
layers_batch_norm = []
for size_input, size_output in zip(sizes, sizes[1:]):
    layer = nn.Linear(size_input, size_output)
    layers_batch_norm.append(layer)
    layers_batch_norm.append(nn.BatchNorm1d(size_output))
    layers_batch_norm.append(nn.ReLU())

# delete last activation
layers_dropout.pop()
# delete last batch normalization
layers_dropout.pop()

network_dropout = TestNetwork(nn.Sequential(*layers_batch_norm))

dropout_output = train(network_dropout, train_loader, test_loader, 50, 0.001, 
                       device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'))

"""#### Feedback (опционально)

Здесь вы можете оставить список опечаток из лекции или семинара:

Здесь вы можете оставить комментарии по лекции или семинару:
"""